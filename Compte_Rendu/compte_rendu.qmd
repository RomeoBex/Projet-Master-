---
title: "Cohérence des attentes pour l'étalonnage des réseaux neuronaux"
author: "Rivaldi Tristan et Bex Roméo"
date: "2024-05-29"
lang: fr
format:
  pdf:
    documentclass: report
    link-citations: true
    toc: true
    toc-title: "Table des matières"
    block-headings: false
    include-in-header:
        - text: |
            \usepackage{dsfont}
            \usepackage{stmaryrd}
            \usepackage{hyperref}
            \usepackage{geometry}
            \renewcommand{\contentsname}{Sommaire}
bibliography: references.bib
---


\newpage

# Introduction et Motivation

Malgré leurs performances incroyables, il est bien connu que les réseaux neuronaux profonds ont tendance à être trop optimistes quant à la confiance qu'ils accordent à leurs prédictions.
Trouver des méthodes de calibration efficaces pour les réseaux neuronaux est donc un effort important pour une meilleure quantification de l'incertitude dans l'apprentissage profond.

Dans ce travail nous considérerons une nouvelle technique de calibration appelée cohérence des attentes (EC) (cf. @clarte2023expectation).
Celle-ci consiste en un redimensionnement post-formation des poids de la dernière couche en imposant que la confiance de validation moyenne coïncide avec la proportion moyenne d'étiquettes correctes.
Il s'agira principalement de vérifier empiriquement que la méthode EC permet d'obtenir des performances d'étalonnage similaires à celles de la mise à l'échelle de la température (TS) (cf. @guo2017calibration) pour différentes architectures de réseaux neuronaux, tout en nécessitant des échantillons de validation et des ressources informatiques similaires.
Le code utiliser dans se projet est disponible dans le référentiel <https://github.com/RomeoBex/Projet-Master->.

Nous avons utilisé les modèles pré-entraînés CIFAR-100 Resnet20, CIFAR-10 Resnet20 et CIFAR-100 Resnet44 provenant du dépôt Github: <https://github.com/chenyaofo/pytorch-cifar-models> puis nous avons trouver la température $T_{\text{EC}}$ et la température TS et nous avons comparer l'Expected Calibration Error (ECE) et le Score de Brier du modèle non-calibrés, du modèle calibré avec $T_{\text{EC}}$ et du modèle calibré avec TS pour mettre en évidence que le modèle original est sur-confiant et que la technique de calibrage appelée cohérence des attentes (EC) améliore les performances du modèle et  permet d'obtenir des performances d'étalonnage similaires à celles de la mise à l'échelle de la température (TS).

# La Regression logistique

La régression logistique est une technique de modélisation statistique utilisée pour prédire la probabilité qu'une variable descriptive binaire prenne l'une des deux valeurs possibles (0 ou 1), que l'on peut noter $Y$ cette variable (elle appartient à $\{0, 1\}^n)$ en fonction d'un ensemble de variables explicatives que l'on note $X = \left[\mathbf{x_1}, \ldots,\mathbf{x_n} \right]^\top \in \mathbb{R}^{n \times p}$, avec $n$ observations et $p$ variables. C'est une méthode couramment utilisée en apprentissage automatique et en statistiques.

La régression logistique a une interprétation probabiliste, elle permet de modéliser $P(Y = 1 | \mathbf{x})$, où $\mathbf{x}\in \mathbb{R}^{p}$.

En utilisant la loi de Bayes et le fait que :

$$
P(\mathbf{x}) = P(\mathbf{x}|Y=1)P(Y=1) + P(\mathbf{x}|Y=0)P(Y=0)\enspace .
$$

Nous avons :

```{=tex}
\begin{align*}
P(Y=1|\mathbf{x})
& = \frac{P(\mathbf{x}|Y=1)P(Y=1)}{P(\mathbf{x}|Y=1)P(Y=1) + P(\mathbf{x}|Y=0)P(Y=0)}\\
& = \frac{1}{1 + \frac{P(\mathbf{x}|Y=0)P(Y=0)}{P(\mathbf{x}|Y=1)P(Y=1)}} \\
& = \frac{1}{1 + \frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}}\enspace .
\end{align*}
```
On note $f(\mathbf{x}) := \log\left(\frac{P(Y=1|\mathbf{x})}{P(Y=0|\mathbf{x})}\right)$.

On a ainsi $P(Y=1|\mathbf{x}) =: sig(f(\mathbf{x}))$ avec $sig(z) = \frac{1}{1 + e^{-z}}$.

La fonction $sig$, appelée fonction logistique ou fonction sigmoïde, satisfait les propriétés suivantes :

$$
sig(-z) = 1 - sig(z)
$$

$$
\frac{dsig(z)}{dz} = sig(z) sig(-z)\enspace .
$$

L'intérêt de la fonction logistique réside dans sa capacité à transformer une fonction $f$ à valeurs dans $\mathbb{R}$ en une probabilité comprise entre 0 et 1.

La régression logistique revient en fait à supposer que $f$ est linéaire de la forme $f : \mathbf{x} \mapsto {\theta}^\top \mathbf{x}$ avec ${\theta} \in \mathbb{R}^p$.

Sous cette hypothèse, la règle de classification est simplement :

$$
\begin{cases}
\text{si } {\theta}^\top \mathbf{x}\leq 0 , \text{ on étiquette 1 au point } \mathbf{x} \\
\text{si } {\theta}^\top \mathbf{x} > 0, \text{ on étiquette 0 au point } \mathbf{x}
\end{cases}
$$

On obtient donc :

$$
P(Y=1|\mathbf{x}) = sig({\theta}^\top \mathbf{x})
$$

$$
P(Y=0|\mathbf{x}) = 1- sig({\theta}^\top \mathbf{x}) = sig(-{\theta}^\top \mathbf{x})\enspace .
$$

Le but maintenant est d'estimer ${\theta}$. Nous avons $(x_i, y_i)_{1 \leq i \leq n}$ où $x_i \in \mathbb{R}^p$ et $y_i \in \{0,1\}$ qui constitue un échantillon de taille $n$. . On a alors :

$$
P(Y = y_i | \mathbf{x} = x_i) = sig({\theta}^\top x_i)^{y_i} sig(-{\theta}^\top x_i)^{1-y_i}\enspace .
$$

La log-vraisemblance s'exprime alors de cette manière :

$$
L({\theta}) = \sum_{i=1}^{n} \log\left( sig({\theta}^\top x_i)^{y_i} sig(-{\theta}^\top x_i)^{1-y_i} \right)\enspace .
$$

Il faut ensuite avoir recours à des algorithmes itératifs (descente de gradient, méthode de Newton,...) pour trouver ${\hat{\theta}}$.

On peut passer du cadre binaire au cadre multi-classes avec $K$ classes, par exemple, c'est-à-dire en ayant $Y$ appartenant à $\{1, 2, \ldots, K\}^n$. De nouveau, on modélise les probabilités conditionnelles des classes, ou plutôt leur log-ratio, par des quantités linéaires, pour $k \in \{1, 2, \ldots, K\}$ et $\theta_{k} \in \mathbb{R}^p$:
 

$$
\log\left(\frac{P(Y=k|\mathbf{x})}{P(Y=K|\mathbf{x})}\right) = \theta_k^\top \mathbf{x}\enspace .
$$


On a alors pour paramètre global: $\theta \in [\theta_1, \ldots, \theta_K] \in \mathbb{R}^{p\times K}$ et pour $k \in \{1, 2, \ldots, K\}$:

$$
P(Y=k | \mathbf{x}) = \frac{\exp(\langle {\theta}_k, \mathbf{x} \rangle)}{\sum_{l=1}^{K} \exp(\langle {\theta}_l, \mathbf{x} \rangle)} \\
= \frac{\exp({\theta}_k^T \mathbf{x})}{\sum_{l=1}^{K} \exp({\theta}_l^T \mathbf{x})}\enspace .
$$

On peut écrire cette égalité sous forme vectorielle en utilisant la notation softmax que l'on note $\sigma$ où :

```{=tex}
\begin{align*}
\sigma : \mathbb{R}^K & \to \mathbb{R}^K \\
(z_1, \dots, z_K) & \mapsto \left(\frac{e^{z_{1}}}{\sum_{j=1}^{K} e^{z_{j}}},\dots, \frac{e^{z_{K}}}{\sum_{j=1}^{K} e^{z_{j}}} \right)\enspace .
\end{align*}
```

On a alors :

$$
(P(Y=k | \mathbf{x}))_{k=1,\cdots,K}=\sigma(\theta_1^T\mathbf{x},\cdots,\theta_K^T\mathbf{x})\enspace .
$$

Pour la régression softmax, la log-vraisemblance peut être exprimée comme suit:

$$
L({\theta}) = \sum_{i=1}^{n} \sum_{k=1}^{K} {1}(y_i = k) \log\left(\frac{e^{\theta_k^\top x_i}}{\sum_{l=1}^{K} e^{\theta_l^\top x_i}}\right)\enspace .
$$

On utilise ensuite des méthodes algorithmiques pour trouver ${\hat{\theta}}$.




# Explication de l'algorithme pour la méthode EC

Le but de cet algorithme est de calculer $T_{\text{EC}}$ pour que la confiance moyenne corresponde à la précision moyenne sur l'ensemble de validation. Pour ce faire, on dispose d'un ensemble de validation $(x_i, y_i)$ pour $i = 1, \ldots, n_{\text{val}}$, et d'un classifieur $\hat{f} : X \rightarrow \mathbb{R}^K$ où les $x_i \in \mathbb{R}^p$ sont les caractéristiques (les variables) et les $y_i \in \llbracket 1, K \rrbracket$ sont les étiquettes de classes associées à ces caractéristiques. L'algorithme calcule les logits $z^{(i)} = f(x_i) \in \mathbb{R}^K$ et produit la sortie $\hat{y}_i = \arg \max_k z_{k}^{(i)}$.

Le classifieur prend en entrée des données (ici les caractéristiques $x_i$ extraites d'une observation) et y attribue des logits $z^{(i)}=(z_{1}^{(i)},...,z_{K}^{(i)})$ où pour tous $k$ appartenant à $\llbracket 1, K \rrbracket$, chaque $z_{k}^{(i)} \in \mathbb{R}$. Pour un $k$ fixé $z_{k}^{(i)}$ correspond au logit (score) associé à la classe $k$. Le fait de produire la sortie $\hat{y}_i = \arg \max_k z_{k}^{(i)}$ signifie que la classe correspondant au logit le plus élevé est choisie comme la prédiction. Pour la classe d'appartenance de $x_i$, l'algorithme choisit de prédire que la classe associée à $x_i$ est $\hat{y}_i$.

Ensuite, l'algorithme calcule la précision moyenne sur l'ensemble de validation:

```{=tex}
\begin{align*}
A_{val} = \frac{1}{n_{\text{val}}} \sum_{i=1}^{n_{\text{val}}} \mathds{1}(y_i = \hat{y}_i)  \enspace .
\end{align*}
```

Les logits sont des valeurs brutes, résultant de la dernière couche d'un réseau de neurones avant l'application d'une fonction d'activation. Ces valeurs brutes ne sont pas normalisées et peuvent être n'importe quel nombre réel.

Cependant, avant d'obtenir les probabilités associées à chaque classe, les logits passent généralement par une fonction d'activation softmax. La fonction softmax transforme les logits en probabilités, produisant une distribution de probabilité sur les classes. Les valeurs résultantes après la fonction softmax seront dans l'intervalle \[0,1\], et leur somme sera égale à 1. La fonction softmax va transformer le vecteur $z^{(i)}$ en un vecteur $z^{(i)'}=\sigma(z^{(i)})$ où :

```{=tex}
\begin{align*}
\sigma : \mathbb{R}^K & \to \mathbb{R}^K \\
(z_1, \dots, z_K) & \mapsto \left(\frac{e^{z_{1}}}{\sum_{j=1}^{K} e^{z_{j}}},\dots, \frac{e^{z_{K}}}{\sum_{j=1}^{K} e^{z_{j}}} \right)\enspace .
\end{align*}
```
On a que $\sigma(z^{(i)})_k$ est la probabilité telle qu’estimée par le réseau, que $x_i$ appartienne à la classe $k$. Pour un $x_i \in \mathbb{R}^p$ la prédiction finale du modèle pour prédire quelle est la classe associée à $x_i$, est alors donnée par $\hat{y}_i=\arg\max_{k}\sigma(z^{(i)})_k$ pour $k$ appartenant à $\llbracket 1, K \rrbracket$ et la confiance de prédiction associée est définie comme étant: $\max_{k}\sigma(z^{(i)})_k$ . De plus, on a bien :

$$
\sum_{k=1}^{K} \sigma(z^{(i)})_k = \frac{\sum_{k=1}^{K} e^{z_{k}^{(i)}}}{\sum_{j=1}^{K} e^{z_{j}^{(i)}}} = 1 \enspace .
$$

Pour trouver $T_{\text{EC}}$, on va devoir prendre $T_{\text{EC}}$ tel que :

$$
\frac{1}{n_{\text{val}}} \sum_{i =1}^{n_{\text{val}}} \max_{k=1}^{K} \sigma \left(\frac{z^{(i)}}{T_{\text{EC}}}\right)_k = A_{\text{val}} \enspace .
$$

De cette manière, $T_{\text{EC}}$ permet à ce que la probabilité maximale d'appartenir à une classe après l'application de la fonction softmax soit en accord avec la précision moyenne sur l’ensemble de validation. De cette manière on a bien que la confiance moyenne correspond à la précision moyenne sur l'ensemble de validation.

Pour obtenir le modèle calibré avec $T_{\text{EC}}$ il faut ensuite simplement diviser le vecteur des logits par $T_{\text{EC}}$ avant de lui appliquer la fonction softmax.

## Dans la pratique

Voici le code pour calculer la température $T_{EC}$:

```{python}
# Il faut utiliser:

from sklearn.metrics import accuracy_score

# Fonction pour trouver la température optimale pour la méthode EC

def find_opt_temp_exp_consist(logits, labels, T_min=0.01, T_max=10.0):
    val_error = 1.0 - accuracy_score(labels, torch.argmax(logits, dim=1))

    def objective(T):
        probas = torch.max(torch.softmax(logits / T, dim=1), dim=1)[0]
        return torch.mean(probas) - (1.0 - val_error)

    res = optimize.root_scalar(objective, bracket=[T_min, T_max])
    return float(res.root)
```

Dans la pratique, le code qui calcule la température $T_{EC}$ est une fonction qu'on applique a un modèle de réseau de neurones et qui prend en entrée :

-   les logits: qui sont les sorties brutes du réseau de neurones avant l'application de la fonction softmax.

-   les labels: qui sont les étiquettes réelles associées aux données

-   $T_{min}$ qui est la température minimale à considérer (0,01)

-   $T_{max}$ qui est la température maximale à considérer (10)


La première étape consiste à calculer l'erreur de classification du modèle à l'aide de la fonction `accuracy_score` de `sklearn`. Cette fonction donne le score de classification, de précision du modèle, par exemple si il vaut 0.5 cela veut dire que le modèle à une prédiction juste une fois sur deux. Pour obtenir l'erreur de classification du modèle on a donc juste à faire 1 moins l'`accuracy_score`. Par exemple si elle vaut 0.30 cela veux dire que dans 70% des cas le modèle associe correctement les étiquettes et les données du modèle.

Ensuite, une fonction objectif est définie, notée `objective(T)`, qui prend la température T comme paramètre. À l'intérieur de cette fonction, les logits sont divisés par la température T, puis passés à travers la fonction softmax. On extrait ensuite les probabilités maximales pour chaque exemple avec l'aide de la fonction `torch.max`. La valeur de retour de la fonction objectif est la moyenne de ces probabilités maximales moins la complémentaire de l'erreur de validation c'est-à-dire la précision moyenne sur l'ensemble de validation. On utilise l'optimiseur `optimize.root_scalar` de la bibliothèque `scipy` pour trouver la racine de cette fonction dans l'intervalle spécifié par $[T_{min}, T_{max}]$.

## Résultats Obtenus

Nous avons utilisé des modèles pré-entraînés provenant du dépôt Github: <https://github.com/chenyaofo/pytorch-cifar-models>. Le modèle CIFAR-100 ResNet20 signifie qu'il s'agit d'un réseau ResNet avec 20 couches entraîné sur la base de données CIFAR-100. Cet ensemble de données contient un total de 60 000 images, réparties en 100 classes différentes, avec 600 images par classe. Il est divisé en un ensemble d'entraînement de 50 000 images et un ensemble de test de 10 000 images. Ce modèle a été entraîné sur l'intégralité de l'ensemble d'entraînement, donc l'ensemble de test contenant 10 000 images a été divisé en deux pour former un ensemble de validation et un ensemble de test, chacun contenant 5 000 images. 

Pour le modèle  CIFAR-100 Resnet20 les valeurs trouvées pour température TS et température $T_{EC}$ sont respectivement 1.436 et 1.445, elle sont donc très proches.L'erreur de classification vaut 0.32. Nous remarquons que les deux méthodes donne des températures supérieures à 1 ce qui est cohérent avec le fait que les réseaux originaux étaient sûr-confiants.

| Modèle             | Score de Brier | ECE   |
|--------------------|----------------|-------|
| Modèle non calibré | 0.455          | 0.104 |
| Calibré avec TS    | 0.436          | 0.030 |
| Calibré avec TEC   | 0.436          | 0.028 |

: Résultats obtenus pour CIFAR-100 Resnet20 {#tbl-first}

On peux voir les résultats obtenus @tbl-first. D'après ces résultats on voit que les deux méthodes améliorent le score Brier et L'Expected Calibration Error et donnent des résultats très similaires.

Pour le modèle CIFAR-100 ResNet44, qui correspond à la même base de données que le modèle précédent mais avec un réseau ResNet de 44 couches. Les valeurs trouvées pour température TS et température $T_{EC}$ sont respectivement 1.71 et 1.76 de plus l'erreur de classification vaut 0.28.


| Modèle             | Score de Brier | ECE   |
|--------------------|----------------|-------|
| Modèle non calibré | 0.442          | 0.147 |
| Calibré avec TS    | 0.405          | 0.048 |
| Calibré avec TEC   | 0.405          | 0.046 |

: Résultats obtenus pour CIFAR-100 Resnet44 {#tbl-s}


D'après les résultats obtenus dans @tbl-s  on voit que les deux méthodes améliorent le score Brier et L'Expected Calibration Error.

Le modèle CIFAR-10 Resnet20  signifie qu'il s'agit d'un réseau ResNet avec 20 couches entraîné sur la base de données CIFAR-10. Cet ensemble de données contient un total de 60 000 images réparties en 10 classes différentes, avec 6 000 images par classe. Cet ensemble de données est divisé en un ensemble d'entraînement de 50 000 images et un ensemble de test de 10 000 images. Ce modèle a été entraîné sur l'intégralité de l'ensemble d'entraînement, donc l'ensemble de test contenant 10 000 images a été divisé en deux pour former un ensemble de validation et un ensemble de test, chacun contenant 5 000 images.

Pour le modèle CIFAR-10 Resnet20 les valeurs trouvées pour température TS et température $T_{EC}$ sont respectivement 1,66 et 1,70 de plus l'erreur de classification vaut 0.079, elle est donc très faible.

| Modèle             | Score de Brier | ECE   |
|--------------------|----------------|-------|
| Modèle non calibré | 0.129          | 0.044 |
| Calibré avec TS    | 0.121          | 0.018 |
| Calibré avec TEC   | 0.121          | 0.016 |

: Résultats obtenus pour CIFAR-10 Resnet20 {#tbl-j}

D'après les résultats obtenus dans @tbl-j, on voit que les deux méthodes améliorent le score Brier et l'Expected Calibration Error, et donnent des résultats meilleurs que le modèle non calibré.



# Erreur de calibration attendue (ECE)

## Définition

L'Expected Calibration Error (ECE) est une mesure d'évaluation de la calibration d'un modèle de réseau de neurones, particulièrement dans le contexte de la classification probabiliste (cf. @guo2017calibration). La calibration se réfère à la justesse des prédictions de probabilité du modèle, c'est-à-dire à quel point les probabilités prédites correspondent aux fréquences réelles des événements.

On dit qu'un algorithme de classification est calibré si la probabilité prédite $\hat{p}$, correspond à la probabilité réelle que la prédiction soit bonne. Ce qui revient mathématiquement à:
$$
P(\hat{y}=y|\hat{p}=p)=p, \quad \forall p\in [0;1]\enspace,
$$
où $\hat{y}$ est la classe prédite et y est la vraie classe. Dans tous les contextes pratiques, atteindre une calibration parfaite est impossible. L'erreur de calibration est une valeur qui représente la calibration du modèle sur l'ensemble des prédictions. Il s'agit de l'espérance mathématique de la différence entre la réalité et la confiance du modèle. On a donc:
$$
ECE=\mathbb{E}[P(\hat{y}=y|\hat{p}=p)-p]_{\hat{p}} \enspace.
$$
On a donc que une valeur faible de l'ECE indique une bonne calibration, tandis qu'une valeur élevée suggère une mauvaise calibration. En effet, une ECE faible indique que le modèle a une tendance à produire des probabilités proches des véritables probabilités d'appartenance à une classe.

La calibration d'un modèle peut être visualisée par un diagramme de fiabilité ( reliability diagram). Pour estimer la précision attendue à partir d'échantillons finis de taille $N$, il faut regrouper les prédictions en M intervalles (chacun de taille $\frac{1}{M}$). On considère l'intervalle $I_m=[\frac{m-1}{M},\frac{m}{M}]$ et $B_m$ l'ensemble des indices des échantillons dont la confiance de prédiction se situe dans l'intervalle $I_m$. Pour chaque groupe $B_m$ on calcule la précision (accuracy) qui correspond à la proportion d'échantillons correctement classés et la confiance moyenne:

$$
acc(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}{1}(y_i=\hat{y_i}) \enspace ,
$$

$$
conf(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}\hat{p_i} \enspace .
$$
Puisqu’il y a un nombre fini M de groupes, on calcule l’erreur de calibration comme suit :

$$
ECE=\frac{1}{N}\sum_{m=1}^{M}|B_m||acc(B_m)-conf(B_m)|\enspace .
$$


On peut donner un exemple assez simple, prenons 9 échantillons avec des probabilités estimées ou aussi appelés « confidences » ($\hat{p_i}$) pour prédire soit 0 ou 1.
Si la probabilité $\hat{p_i}$ pour l'étiquette 0 est supérieure à 0,5 alors l'étiquette prédite sera 0. S'il est inférieur à 0,5, alors la probabilité sera plus élevée pour l'étiquette 1 et, par conséquent, l'étiquette prévue sera 1 (voir @tbl-second). La dernière colonne montre la véritable étiquette d'un échantillon i.



| échantillon (i) | Probabilité prédite pour le label 0| Probabilité prédite pour le label 1 | label prédit ($\hat{y_i}$) | Vrai label ($y_i$) |
|-----------------|------------------------------------|-------------------------------------|----------------------------|--------------------|
| 1               | 0.78                               | 0.22                                |  0                         |   0                |
| 2               | 0.36                               | 0.64                                |  1                         |   1                |
| 3               | 0.08                               | 0.92                                |  1                         |   0                |
| 4               | 0.58                               | 0.42                                |  0                         |   0                |
| 5               | 0.49                               | 0.51                                |  1                         |   0                |
| 6               | 0.85                               | 0.15                                |  0                         |   0                |
| 7               | 0.30                               | 0.70                                |  1                         |   1                |
| 8               | 0.63                               | 0.37                                |  0                         |   1                |
| 9               | 0.17                               | 0.83                                |  1                         |   0                |

: Situation {#tbl-second}

Pour déterminer le reste de la formule, nous devrons d'abord diviser nos échantillons en bacs. Seules les probabilités qui déterminent l'étiquette prévue sont utilisées dans le calcul de la ECE. Par conséquent, nous ne déposerons que des échantillons basés sur la probabilité maximale entre les étiquettes (voir @tbl-second). 

Pour garder l'exemple simple, nous avons divisé les données en 5 bacs, $M=5$. Avec B1 qui correspond aux probabilités prédites de 0 à 0.2, B2 qui correspond aux probabilités prédites de 0.2 à 0.4, B3 qui correspond aux probabilités prédites de 0.4 à 0.6, B4 qui correspond aux probabilités prédites de 0.6 à 0.8 et B5 qui correspond aux probabilités prédites de 0.8 à 1.

Déposons maintenant les échantillons dans leurs bacs : dans B1 et B2 il n'y a aucun échantillons, dans B3 on a 4 et 5, dans B4 on a 8, 7, 2 et 1 et dans B5 on a 9, 6 et 3.

On va maintenant déterminer $conf(B_m)$, pour calculer $conf(B_m)$ nous prenons la somme des probabilités maximales estimées $\hat{p_i}$ pour chaque bac m dans @tbl-second, puis nous la divisons par la taille du bac |Bm|, on obtient : $con(B3)=\frac{0.58+0.51}{2}=0.545$, $con(B4)=\frac{0.78+0.64+0.7+0.63}{4}=0.6865$ et $con(B5)=\frac{0.92+0.85+0.83}{3}=0.8667$.

Il ne nous manque plus que $acc(B_m)$ qui correspond au nombre d'échantillons correctement prédit dans le bac m diviser par le nombre d'échantillons dans le bac. On trouve donc : $acc(B3)=0.5$, $acc(B4)=0.75$ et $acc(B5)=\frac{2}{3}$.

On peut don maintenant calculer l'ECE :

$$
ECE= \frac{2}{9}|0.5-0.545|   +   \frac{4}{9}|0.75-0.6865| + \frac{3}{9}|\frac{2}{3}-0.8667|=0.1044
$$
Dans notre petit exemple de 9 échantillons, nous nous avons une ECE de 0,1044. Un modèle parfaitement calibré aurait une ECE de 0.


## Diagrammes de fiabilité

On peut visualiser graphiquement cela à quoi ca correspond avec l'aide des diagrammes de fiabilité  (reliability diagrams en anglais).
Notre but ici est de montrer que le modèle calibrer avec la température $T_{EC}$ a une erreur de calibration plus faible que le modèle non calibré et une erreur de calibration relativement proche du modèle calibré avec TS. Voici les résultats pour un nombre de Bacs $M=10$:


::: {#fig-ECE layout-ncol=3 fig-pos='H'}
![Modèle non calibré](Figure_1.svg){width=170 #fig-1 }

![Modèle calibré avec TS](Figure_2.svg){width=170 #fig-2 }

![Modèle calibré avec TEC](Figure_3.svg){width=170 #fig-3 }

Diagrammes de fiabilité pour Cifar100-resnet20 
:::


En comparant @fig-1 avec @fig-2 et @fig-3 on remarque que c'est bien le cas. Pour interpréter ces Figures, on voit que sur @fig-1 le modèle a tendance a être sur-confiant, car les probabilités qu'il prédit (confidence) sont supérieurs à l'Expected accuracy qui est la précision réelle du modèle. Un modèle parfaitement calibré reste sur la diagonale, les barres rouges montre l'écart entre la confiance et la précision du modèle.

Pour la @fig-1 on voit que la confiance moyenne est largement supérieur à la précision moyenne du modèle ce qui montre clairement que le modèle est sur-confiant. Tandis qu'on voit que sur les @fig-2 et @fig-3  le modèle est mieux calibré et il donne des prédictions plus cohérentes au vu de la précision du modèle.

On obtient pour Cifar100-resnet44 :





::: {#fig-ECE layout-ncol=3 fig-pos='H'}
![Modèle non calibré](figure_1_44_nc.svg){width=170 #fig-7}

![Modèle calibré avec TS](Figure_2_44_ts.svg){width=170 #fig-8}

![Modèle calibré avec TEC](Figure_3_44_ec.svg){width=170 #fig-9}

Diagrammes de fiabilité pour Cifar100-resnet44 
:::

En comparant @fig-7 avec @fig-8 et @fig-9, on voit que pour Cifar100-resnet44, les deux méthodes d'étalonnage améliorent la calibration du modèle et donnent des résultats très similaires, mais légèrement meilleurs pour la méthode EC. Les deux méthodes permettent presque de faire correspondre la confiance moyenne du modèle et la précision réelle du modèle.

On a pour Cifar10-resnet20 :

::: {#fig-ECE layout-ncol=3 fig-pos='H'}
![Modèle non calibré](CIFAR10_NC.svg){width=170 #fig-4}

![Modèle calibré avec TS](CIFAR10_TS.svg){width=170 #fig-5}

![Modèle calibré avec TEC](CIFAR10_EC.svg){width=170 #fig-6}

Diagrammes de fiabilité pour Cifar10-resnet20 
:::



En comparant @fig-4 avec @fig-5 et @fig-6, on voit que pour ce modèle, les deux méthodes d'étalonnage améliorent la calibration du modèle. Elles améliorent l'ECE.



# Proprités mathématiques

Nous nous appuierons sur cette article (cf. @rigollet2012sparse). Nous avons vu que pour un problème de classification en K classes, dans le cadre d'un modèle calibré avec une température $T>0$. Pour une image $y_i$ la fonction softmax va lui associer un vecteur de probabilités de la forme : 

$$
\left(\frac{e^{z_{1}/T}}{\sum_{j=1}^{K} e^{z_{j}/T}},\dots, \frac{e^{z_{K/T}}}{\sum_{j=1}^{K} e^{z_{j/T}}} \right)
$$
où $z_k$ correspond pour tout $k$ appartenant à $\llbracket 1, K \rrbracket$ au score associé à la classe k. On pose :
$$
\sigma_{k}= \frac{e^{z_{k}/T}}{\sum_{j=1}^{K} e^{z_{j}/T}}\enspace.
$$
On considère le vecteur $z=(z_{1},...,z_{K})$ et on définit le simplexe suivant :

$$
\Lambda_{K} := \left\{ z \in \mathbb{R}^{K} :z_{k} \geq 0, \sum_{j=1}^{K} z_{k}=1 \right\}\enspace.
$$

On définit $\lambda=(\lambda_1,...,\lambda_K)$ où $\lambda \in \Lambda_{K}$ qui est donc par définition une mesure de probabilité sur $\{1, \ldots, K\}$ et $\pi=(\frac{1}{K},...,\frac{1}{K})\in \mathbb{R}^{K}$ ($\pi$ suit une distibution uniforme sur $\{1, \ldots, K\}$). La divergence de Kullback-Leibler entre $\lambda$ et $\pi$ peut être vue comme une sorte de distance (même si elle n'est pas symétrique) entre deux distributions, ici $\lambda$ et $\pi$. Elle est définie comme : 

$$
K(\lambda,\pi)=\sum_{j=1}^{K} \lambda_j\log(\frac{\lambda_j}{\pi_j})\enspace.
$$

On a alors dans ce cas :

$$
\left(\sigma_1,\dots, \sigma_K \right)=\underset{\lambda \in \Lambda_{K}}{\text{argmin}} \left\{ -\sum_{j=1}^{K}\lambda_jz_{j}+T \cdot K(\lambda,\pi) \right\}
$$

Pour prouver cela il nous faut résoudre un problème d'optimisation convexe, en effet le critère à minimiser et l'ensemble admissible sont convexe. Il existe donc une unique solution à ce problème que l'on notera $\left(\sigma_1,\dots, \sigma_K \right)$.

On pose :  
$$
\begin{cases}
    g(\sigma_1, \dots , \sigma_K) = \sum_{j=1}^{K}\sigma_j - 1 \\
    h_j(\sigma_1, \dots , \sigma_K) = -\lambda_j \quad \text{pour } j \in \llbracket 1,K \rrbracket \\
    f(\sigma_1, \dots , \sigma_K) = -\sum_{j=1}^{K}\sigma_{j}z_{j} + T \cdot K(\lambda, \pi) 
\end{cases}
$$


Le Lagrangien devient : 

$$
L(\sigma_1, \dots , \sigma_K,\lambda,\mu) = f(\sigma_1, \dots , \sigma_K) - \delta g(\sigma_1, \dots , \sigma_K) - \mu_1 h_1(\sigma_1, \dots , \sigma_K) - ... - \mu_{K}h_{K}(\sigma_1, \dots , \sigma_K)
$$ 

et on obtient 
$$
\frac{\partial L}{\partial \sigma_i}(\sigma_1, \dots , \sigma_K) = -z_i + T log (\frac{\sigma_i}{\frac{1}{K}}) - \delta_i + \mu_i = 0
$$

$$
log(\sigma_{i}K) = \frac{z_i + \delta_i - \mu_i}{T}
$$

De plus, en utilisant notre contrainte de complémentarité $\sigma_i\delta_i = 0$ et le fait que $\sum_{i=1}^{K}\sigma_i = 1$ on a : 


```{=tex}
\begin{align*}
     \sigma_i &= \frac{\exp\left(\frac{z_i + \delta_i - \mu_i}{T}\right)}{K} \\
    &= \frac{\frac{\exp\left(\frac{z_i + \delta_i - \mu_i}{T}\right)}{K}}{\frac{1}{K}\sum_{i=1}^{K}\exp\left(\frac{z_i + \delta_i - \mu_i}{T}\right)} \\
    &= \frac{exp(\frac{z_i + \delta_i - \mu_i}{T})}{\sum_{i=1}^{K} exp(\frac{z_i + \delta_i - \mu_i}{T})} \\ 
    &= \frac{exp(\frac{z_i}{T})}{\sum_{i=1}^{K} exp(\frac{z_i}{T})}
\end{align*}

```

et 
```{=tex}
\[
\boxed{(\sigma_1, \dots , \sigma_K) = \left(\frac{\exp\left(\frac{z_1}{T}\right)}{\sum_{j=1}^{K} \exp\left(\frac{z_i}{T}\right)}, \dots ,\frac{\exp\left(\frac{z_K}{T}\right)}{\sum_{j=1}^{K} \exp\left(\frac{z_K}{T}\right)}\right)}
\]
```



On trouve donc que quand $T \to 0^+$, 


$$
\left(\sigma_1,\dots, \sigma_K \right)\to \underset{\lambda \in \Lambda_{K}}{\text{argmax}} \left\{ \sum_{j=1}^{K}\lambda_jz_{j} \right\}
$$ 
donc $\left(\sigma_1,\dots, \sigma_K \right)\to \delta_{z_{max}}$, où $\delta$ est la fonction Dirac. De plus, quand $T \to \infty$, 

$$
\left(\sigma_1,\dots, \sigma_K \right)\to \underset{\lambda \in \Lambda_{K}}{\text{argmin}} \left\{ K(\lambda,\pi)  \right\}
$$
donc $\left(\sigma_1,\dots, \sigma_K \right)\to \pi$.

Donc si $T ≪ 1$, le softmax sera dominé par la classe ayant la plus grande confiance. Cela conduira généralement à un prédicteur surconfiant. D'autre part, pour $T ≫ 1$, le softmax sera de moins en moins sensible aux poids entraînés et convergera vers un vecteur uniforme quand $T \to \infty$. Cela correspondra généralement à un prédicteur sous-confiant. Par conséquent, en réglant T, nous pouvons rendre un prédicteur moins sous-confiant (en abaissant la température T < 1) ou moins sur-confiant (en augmentant la température T > 1).






# Score de Brier

Le score de Brier est une fonction de Score qui mesure l'exactitude des prédictions probabilistes. Pour les prédictions unidimensionnelles, elle est strictement équivalente à l'erreur quadratique moyenne aux probabilités prédites.

## Définition générale 

Dans le cas où une variable peut prendre plus de 2 valeurs. Le score de Brier est alors défini par :

$$
B_s = \frac{1}{n}\sum_{i=1}^{m}\sum_{j=1}^{n_k}(f_{i,j} - o_{i,j})^2
$$

- $m$ représente le nombre total d'instances de toutes les classes (nombre de sous vecteurs)
- $n_k$ représente le nombre total de classes possibles dans lesquels l'évènement peut tomber
- $f_{i,j}$ représente la probabilité prédite pour la classe $i$ pour $i$ $\in \llbracket 1,n\rrbracket$
- $o_{i,j}$ vaut 1 si la ième observation est de la catégorie $j$ et 0 sinon pour $j$
$\in \llbracket 1,m\rrbracket$

Le score de Brier peut être décomposé en 3 composantes additives : incertitude, fiabilité et résolution

En effet, par définition on a : 

$$
B_s = \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}(f_{kj}-o_{kj})^2 
= \frac{1}{n}\sum_{k=1}^{m}n_k[\ \frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj})^2]\ \enspace .
$$

Le terme entre crochet peut être vu comme la moyenne d'une grandeur au carré et peut être donc décomposée comme la somme de la quantité moyenne au carré plus la variance de la quantité. Ainsi on a : 

$$
\frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj})^2 = \frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj} - \frac{1}{n_k}\sum_{j=1}^{n_k}(f_kj - o_{kj}))^2 + (\frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj}))^2
$$ 

qui peut être écrit en utilisant la linéarité de la somme et en notant :

$$
\begin{cases}
\bar{o}_k = \frac{1}{n_k}\sum_{j=1}^{n_k}o_{kj} \\
\bar{f}_k = \frac{1}{n_k}\sum_{j=1}^{n_k}f_{kj} \\
\end{cases}
$$

$$
\frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj})^2 = \frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj} - \bar{f}_k + \bar{o}_k)^2 \enspace .
$$

Ainsi on peut réécrire le Brier Score comme : 

$$
B_s = \frac{1}{n}\sum_{k=1}^{m}n_k \left[ (\bar{f}_k - \bar{o}_k)^2 + \frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj}-o_{kj} - \bar{f}_k + \bar{o}_k)^2 \right] = \frac{1}{n} \sum_{k=1}^{m} \left[ (\bar{f}_k - \bar{o}_k)^2 + \frac{1}{n_k} \sum_{j=1}^{n_k} ((f_{kj} - \bar{f}_k) - (o_{kj} - \bar{o}_k))^2 \right]\enspace .
$$

En développant le carré on a : 

$$ 
B_s = \frac{1}{n} \sum_{k=1}^{m} n_k \left[ (\bar{f}_k - \bar{o}_k)^2 + \frac{1}{n_k}\sum_{j=1}^{n_k}(o_{kj} - \bar{o}_k)^2 + \frac{1}{n_k}\sum_{j=1}^{n_k}(f_{kj} - \bar{f}_k)^2 - \frac{2}{n_k}\sum_{j=1}^{n_k} (f_{kj} - \bar{f}_k)(o_{kj} - \bar{o}_k) \right]\enspace .
$$

Le premier terme de cette somme (REL) est défini par :
$$
\frac{1}{n} \sum_{k=1}^{m} n_k(\bar{f}_k - \bar{o}_k)^2 
$$

c'est le terme de fiabilité des prévisions, il compare les probabilités prévues aux probabilités d'observations.

Le deuxième terme de cette somme est défini par $UNC - RES$ (est obtenu en ajoutant $\bar{o}$ puis en soustrayant $\bar{o}$) :

$$
\frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_k(o_{kj} - \bar{o}_k)^2 = \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_k(o_{kj}-\bar{o})^2 - \frac{1}{n}\sum_{k=1}^{m}n_{k}(o_k - \bar{o})^2\enspace .
$$

De plus, en utilisant le fait que $o_{kj}^2 = o_{kj}$ et que $\bar{o}_k = \frac{1}{n_k}\sum_{j=1}^{n_k}o_{kj}$ on a :

```{=tex}
\begin{align*}
\frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_k(o_{kj}-\bar{o})^2
& = \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_k(o_{kj}^2 - 2o_{kj}\bar{o} + \bar{o}^2) \\
& = \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_{k}o_{kj} - 2n_{k}o_{kj}\bar{o} + n_{k}\bar{o}\\
& = \frac{1}{n}\sum_{k=1}^{m}n_{k}\bar{o}_{k} - \frac{2\bar{o}}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}o_{kj} + \frac{\bar{o}}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}n_k \\ 
& = \bar{o} - 2\bar{o}^2 + \bar{o}^2 = \bar{o}(1-\bar{o})
\end{align*}
```

Ainsi,

$$
B_s = \frac{1}{n}n_{k}(\bar{f}_k - \bar{o}_k)^2 + \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{m}(o_{kj}-\bar{o}_k)^2 + \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{m}(f_{kj}-\bar{f}_k)^2 - \frac{2}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}(f_{kj} - \bar{f}_k)(o_{kj} - \bar{o}_k)
$$

Ce qui donne comme décomposition finale : 

 
```{=tex}
\[
\boxed{B_s = \frac{1}{n}\sum_{k=1}^{m}n_{k}(\bar{f}_k - \bar{o}_k)^2 - \frac{1}{n}\sum_{k=1}^{m}n_{k}(\bar{o}_k - \bar{o})^2 + \bar{o}(1-\bar{o}) + \frac{1}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}(f_k - \bar{f}_k)^2 - \frac{2}{n}\sum_{k=1}^{m}\sum_{j=1}^{n_k}(f_{kj} - \bar{f}_k)(o_{kj}- \bar{o}_k)}
\]
```

D'où :

$$
B_s = REL - RES + UNC + WBV - WBC
$$

- $WBV$ variance intra-groupe et $WBC$ covariance intra-groupe, ces termes disparaissent si une seule valeur de probabilité est prévue pour chaque groupe (on a $f_{kj} = f_{k}$)
- $UNC$ incertitude des observations 
- $RES$ résolution prévisionnelle 
- $REL$ fiabilité des prévisions 




## Interprétation et décomposition 

Plus la valeur du score de Brier sera faible plus la prédiction sera bonne et une prévision parfaite obtiendra un score de 0. A l'inverse le plus mauvais score sera de 1.

Code de la fonction :


```{python}
# code de la fonction


def brier_score_f(predictions, observations):
    """
    Calcul du score de Brier.

    Arguments:
    predictions (liste de liste ou numpy array) Prédictions des probabilités
    pour chaque classe.
    observations (liste de liste ou numpy arrays): Observations réelles
    (0 ou 1 pour chaque classe).

    """
    n = len(predictions)
    m = len(predictions[0])
    bs = 0.0

    for i in range(n):
        for j in range(m):
            bs += (predictions[i][j] - observations[i][j]) ** 2

    bs /= (n * m)
    return bs

```

Exécution :

```{python}
# Exemple :

predictions = [[0.1, 0.9, 0.8, 0.3],[0.4,0.9,0.2,0.7]]
observations = [[0, 1, 1, 0],[1,1,0,1]]

bs = brier_score_f(predictions, observations)
print("Score de Brier :", bs)


```



# Conclusion 


La méthode EC est une technique d'étalonnage post-entraînement qui peut presque parfaitement restaurer l'étalonnage de certains modèles de réseaux neuronaux.

Elle permet d'obtenir des performances d'étalonnage très similaires à celles de la mise à l'échelle de la température (TS). 
D'après nos observations, pour les modèles Cifar100-resnet20, Cifar100-resnet44 et Cifar10-resnet20  elle permet de presque parfaitement restaurer l'étalonnage du réseau.


Pour conclure, si vous avez un modèle entraîné qui est en moyenne beaucoup plus confiant que les performances réelles du modèle, vous pouvez procéder à la méthode d'étalonnage EC et vous obtiendrez des résultats très similaires à la méthode de mise à l'échelle de la température (TS). Vous obtiendrez un modèle mieux calibré.


# Bibliographie

::: {#refs}
:::
