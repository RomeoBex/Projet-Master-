---
title: "Cohérence des attentes pour l'étalonnage des réseaux neuronaux"
author: "Rivaldi Tristan et Bex Roméo"
date: "2024-05-29"
lang: fr
format:
  pdf:
    documentclass: report
    link-citations: true
    toc: true
    toc-title: "Table des matières"
    block-headings: false
    include-in-header:
        - text: |
            \usepackage{dsfont}
            \usepackage{stmaryrd}
            \usepackage{hyperref}
            \usepackage{geometry}
            \renewcommand{\contentsname}{Sommaire}
bibliography: references.bib
---


\newpage

# Introduction et Motivations

Malgré leurs performances incroyables, il est bien connu que les réseaux neuronaux profonds ont tendance à être trop optimistes quant à la confiance qu'ils accordent à leurs prédictions.
Trouver des méthodes de calibration efficaces pour les réseaux neuronaux est donc un effort important pour une meilleure quantification de l'incertitude dans l'apprentissage profond.

Dans ce travail nous considérerons une nouvelle technique de calibration appelée cohérence des attentes (EC) (cf. @clarte2023expectation).
Celle-ci consiste en un redimensionnement post-formation des poids de la dernière couche en imposant que la confiance de validation moyenne coïncide avec la proportion moyenne d'étiquettes correctes.
Il s'agira principalement de vérifier empiriquement que la méthode EC permet d'obtenir des performances d'étalonnage similaires à celles de la mise à l'échelle de la température (TS) (cf. @guo2017calibration) pour différentes architectures de réseaux neuronaux, tout en nécessitant des échantillons de validation et des ressources informatiques similaires.
Le code utiliser dans se projet est disponible dans le référentiel <https://github.com/RomeoBex/Projet-Master->.

Nous avons utilisé le modèle pré-entraîné CIFAR-100 Resnet20 provenant du dépôt Github: <https://github.com/chenyaofo/pytorch-cifar-models> puis nous avons trouver la température $T_{\text{EC}}$ et la température TS et nous avons comparer l'Expected Calibration Error (ECE) et le Score de Brier du modèle non-calibrés, du modèle calibré avec $T_{\text{EC}}$ et du modèle calibré avec TS pour mettre en évidence que le modèle original est sur-confiant et que la technique de calibrage appelée cohérence des attentes (EC) améliore les performances du modèle et  permet d'obtenir des performances d'étalonnage similaires à celles de la mise à l'échelle de la température (TS).

# La Regression logistique

La régression logistique est une technique de modélisation statistique utilisée pour prédire la probabilité qu'une variable descriptive binaire prenne l'une des deux valeurs possibles (0 ou 1), que l'on peut noter $Y$ cette variable (elle appartient à $\{0, 1\}^n)$ en fonction d'un ensemble de variables explicatives que l'on note $X = \left[\mathbf{x_1}, \ldots,\mathbf{x_n} \right]^\top \in \mathbb{R}^{n \times p}$, avec $n$ observations et $p$ variables. C'est une méthode couramment utilisée en apprentissage automatique et en statistiques.

La régression logistique a une interprétation probabiliste, elle permet de modéliser $P(Y = 1 | \mathbf{x})$, où $\mathbf{x}\in \mathbb{R}^{p}$.

En utilisant la loi de Bayes et le fait que :

$$
P(\mathbf{x}) = P(\mathbf{x}|Y=1)P(Y=1) + P(\mathbf{x}|Y=0)P(Y=0)
$$

nous avons :

```{=tex}
\begin{align*}
P(Y=1|\mathbf{x})
& = \frac{P(\mathbf{x}|Y=1)P(Y=1)}{P(\mathbf{x}|Y=1)P(Y=1) + P(\mathbf{x}|Y=0)P(Y=0)}\\
& = \frac{1}{1 + \frac{P(\mathbf{x}|Y=0)P(Y=0)}{P(\mathbf{x}|Y=1)P(Y=1)}} \\
& = \frac{1}{1 + \frac{P(Y=0|\mathbf{x})}{P(Y=1|\mathbf{x})}}
\end{align*}
```
On note $f(\mathbf{x}) := \log\left(\frac{P(Y=1|\mathbf{x})}{P(Y=0|\mathbf{x})}\right)$.

On a ainsi $P(Y=1|\mathbf{x}) =: sig(f(\mathbf{x}))$ avec $sig(z) = \frac{1}{1 + e^{-z}}$.

La fonction $sig$, appelée fonction logistique ou fonction sigmoïde, satisfait les propriétés suivantes :

$$
sig(-z) = 1 - sig(z)
$$

$$
\frac{dsig(z)}{dz} = sig(z) sig(-z)
$$

L'intérêt de la fonction logistique réside dans sa capacité à transformer une fonction $f$ à valeurs dans $\mathbb{R}$ en une probabilité comprise entre 0 et 1.

La régression logistique revient en fait à supposer que $f$ est linéaire de la forme $f : \mathbf{x} \mapsto {\theta}^\top \mathbf{x}$ avec ${\theta} \in \mathbb{R}^p$.

Sous cette hypothèse, la règle de classification est simplement :

$$
\begin{cases}
\text{si } {\theta}^\top \mathbf{x}\leq 0 , \text{ on étiquette 0 au point } \mathbf{x} \\
\text{si } {\theta}^\top \mathbf{x} > 0, \text{ on étiquette 1 au point } \mathbf{x}
\end{cases}
$$

On obtient donc :

$$
P(Y=1|\mathbf{x}) = sig({\theta}^\top \mathbf{x})
$$

$$
P(Y=0|\mathbf{x}) = 1- sig({\theta}^\top \mathbf{x}) = sig(-{\theta}^\top \mathbf{x})
$$

Le but maintenant est d'estimer ${\theta}$. Nous avons $(x_i, y_i)_{1 \leq i \leq n}$ où $x_i \in \mathbb{R}^p$ et $y_i \in \{0,1\}$ qui constitue un échantillon de taille $n$. . On a alors :

$$
P(Y = y_i | \mathbf{x} = x_i) = sig({\theta}^\top x_i)^{y_i} sig(-{\theta}^\top x_i)^{1-y_i}
$$

La log-vraisemblance s'exprime alors de cette manière :

$$
L({\theta}) = \sum_{i=1}^{n} \log\left( sig({\theta}^\top x_i)^{y_i} sig(-{\theta}^\top x_i)^{1-y_i} \right)
$$

où $l$ est définie comme étant la fonction de perte logistique. Il faut ensuite avoir recours à des algorithmes itératifs (descente de gradient, méthode de Newton,...) pour trouver ${\hat{\theta}}$.

On peut passer du cadre binaire au cadre multi-classes avec $K$ classes, par exemple, c'est-à-dire en ayant $Y$ appartenant à $\{1, 2, \ldots, K\}$. De nouveau, on modélise les probabilités conditionnelles des classes, ou plutôt leur log-ratio, par des quantités linéaires, pour $k \in \{1, 2, \ldots, K-1\}$ et $\theta_{k} \in \mathbb{R}^p$:


$$
\log\left(\frac{P(Y=k|\mathbf{x})}{P(Y=K|\mathbf{x})}\right) = \theta_k^\top \mathbf{x}
$$


On a alors pour paramètre global: $\theta \in [\theta_1, \ldots, \theta_K] \in \mathbb{R}^{p\times K}$ et pour $k \in \{1, 2, \ldots, K\}$:

$$
P(Y=k | \mathbf{x}) = \frac{\exp(\langle {\theta}_k, \mathbf{x} \rangle)}{\sum_{l=1}^{K} \exp(\langle {\theta}_l, \mathbf{x} \rangle)} \\
= \frac{\exp({\theta}_k^T \mathbf{x})}{\sum_{l=1}^{K} \exp({\theta}_l^T \mathbf{x})}
$$

On peut écrire cette égalité sous forme vectorielle en utilisant la notation softmax, on a alors :

$$
(P(Y=k | \mathbf{x}))_{k=1,\cdots,K}=softmax(\theta_1^T\mathbf{x},\cdots,\theta_K^T\mathbf{x})
$$

Pour la régression softmax, la log-vraisemblance peut être exprimée comme suit:

$$
L({\theta}) = \sum_{i=1}^{n} \sum_{k=1}^{K} {1}(y_i = k) \log\left(\frac{e^{\theta_k^\top x_i}}{\sum_{l=1}^{K} e^{\theta_l^\top x_i}}\right)
$$

On utilise ensuite des méthodes algorithmiques pour trouver ${\hat{\theta}}$.




# Explication de l'algorithme pour la méthode EC

Le but de cet algorithme est de calculer $T_{\text{EC}}$ pour que la confiance moyenne corresponde à la précision moyenne sur l'ensemble de validation. Pour ce faire, on dispose d'un ensemble de validation $(x_i, y_i)$ pour $i = 1, \ldots, n_{\text{val}}$, et d'un classifieur $\hat{f} : X \rightarrow \mathbb{R}^K$ où les $x_i \in \mathbb{R}^p$ sont les caractéristiques (les variables) et les $y_i \in \llbracket 1, K \rrbracket$ sont les étiquettes de classes associées à ces caractéristiques. L'algorithme calcule les logits $z^{(i)} = f(x_i) \in \mathbb{R}^K$ et produit la sortie $\hat{y}_i = \arg \max_k z_{k}^{(i)}$.

Le classifieur prend en entrée des données (ici les caractéristiques $x_i$ extraites d'une observation) et y attribue des logits $z^{(i)}=(z_{1}^{(i)},...,z_{K}^{(i)})$ où pour tous $k$ appartenant à $\llbracket 1, K \rrbracket$, chaque $z_{k}^{(i)} \in \mathbb{R}$. Pour un $k$ fixé $z_{k}^{(i)}$ correspond au logit (score) associé à la classe $k$. Le fait de produire la sortie $\hat{y}_i = \arg \max_k z_{k}^{(i)}$ signifie que la classe correspondant au logit le plus élevé est choisie comme la prédiction. Pour la classe d'appartenance de $x_i$, l'algorithme choisit de prédire que la classe associée à $x_i$ est $\hat{y}_i$.

Ensuite, l'algorithme calcule la précision moyenne sur l'ensemble de validation:

```{=tex}
\begin{align*}
A_{val} = \frac{1}{n_{\text{val}}} \sum_{i} \mathds{1}(y_i = \hat{y}_i)  \enspace .
\end{align*}
```

Les logits sont des valeurs brutes, résultant de la dernière couche d'un réseau de neurones avant l'application d'une fonction d'activation. Ces valeurs brutes ne sont pas normalisées et peuvent être n'importe quel nombre réel.

Cependant, avant d'obtenir les probabilités associées à chaque classe, les logits passent généralement par une fonction d'activation softmax. La fonction softmax transforme les logits en probabilités, produisant une distribution de probabilité sur les classes. Les valeurs résultantes après la fonction softmax seront dans l'intervalle \[0,1\], et leur somme sera égale à 1. La fonction softmax va transformer le vecteur $z^{(i)}$ en un vecteur $z^{(i)'}=\sigma(z^{(i)})$ où :

```{=tex}
\begin{align*}
\sigma : \mathbb{R}^K & \to \mathbb{R}^K \\
(z_1, \dots, z_K) & \mapsto \left(\frac{e^{z_{1}}}{\sum_{j=1}^{K} e^{z_{j}}},\dots, \frac{e^{z_{K}}}{\sum_{j=1}^{K} e^{z_{j}}} \right)\enspace .
\end{align*}
```
On a que $\sigma(z^{(i)})_k$ est la probabilité telle qu’estimée par le réseau, que $x_i$ appartienne à la classe $k$. Pour un $x_i \in \mathbb{R}^p$ la prédiction finale du modèle pour prédire quelle est la classe associée à $x_i$, est alors donnée par $\hat{y}_i=\arg\max_{k}\sigma(z^{(i)})_k$ pour $k$ appartenant à $\llbracket 1, K \rrbracket$ et la confiance de prédiction associée est définie comme étant: $\max_{k}\sigma(z^{(i)})_k$ . De plus, on a bien :

$$
\sum_{k=1}^{K} \sigma(z^{(i)})_k = \frac{\sum_{k=1}^{K} e^{z_{k}^{(i)}}}{\sum_{j=1}^{K} e^{z_{j}^{(i)}}} = 1 \enspace .
$$

Pour trouver $T_{\text{EC}}$, on va devoir prendre $T_{\text{EC}}$ tel que :

$$
\frac{1}{n_{\text{val}}} \sum_{i =1}^{n_{\text{val}}} \max_{k=1}^{K} \sigma \left(\frac{z^{(i)}}{T_{\text{EC}}}\right)_k = A_{\text{val}} \enspace .
$$

De cette manière, $T_{\text{EC}}$ permet à ce que la probabilité maximale d'appartenir à une classe après l'application de la fonction softmax soit en accord avec la précision moyenne sur l’ensemble de validation. De cette manière on a bien que la confiance moyenne correspond à la précision moyenne sur l'ensemble de validation.

## Dans la pratique

Voici le code pour calculer la température $T_{EC}$:

```{python}
# Il faut utiliser:

from sklearn.metrics import accuracy_score

# Fonction pour trouver la température optimale pour la méthode EC

def find_opt_temp_exp_consist(logits, labels, T_min=0.01, T_max=10.0):
    val_error = 1.0 - accuracy_score(labels, torch.argmax(logits, dim=1))

    def objective(T):
        probas = torch.max(torch.softmax(logits / T, dim=1), dim=1)[0]
        return torch.mean(probas) - (1.0 - val_error)

    res = optimize.root_scalar(objective, bracket=[T_min, T_max])
    return float(res.root)
```

Dans la pratique, le code qui calcule la température $T_{EC}$ est une fonction qu'on applique a un modèle de réseau de neurones et qui prend en entrée :

-   les logits: qui sont les sorties brutes du réseau de neurones avant l'application de la fonction softmax.

-   les labels: qui sont les étiquettes réelles associées aux données

-   $T_{min}$ qui est la température minimale à considérer (0,01)

-   $T_{max}$ qui est la température maximale à considérer (10)


La première étape consiste à calculer l'erreur de classification du modèle à l'aide de la fonction `accuracy_score` de `sklearn`. Cette fonction donne le score de classification, de précision du modèle, par exemple si il vaut 0.5 cela veut dire que le modéle à une prédiction juste une fois sur deux. Pour obtenir l'erreur de classification du modèle on a donc juste à faire 1 moins l'`accuracy_score`. Par exemple si elle vaut 0.30 cela veux dire que dans 70% des cas le modèle associe correctement les étiquettes et les données du modèle.

Ensuite, une fonction objectif est définie, notée `objective(T)`, qui prend la température T comme paramètre. À l'intérieur de cette fonction, les logits sont divisés par la température T, puis passés à travers la fonction softmax. On extrait ensuite les probabilités maximales pour chaque exemple avec l'aide de la fonction `torch.max`. La valeur de retour de la fonction objectif est la moyenne de ces probabilités maximales moins la complémentaire de l'erreur de validation c'est-à-dire la précision moyenne sur l'ensemble de validation. On utilise l'optimiseur `optimize.root_scalar` de la bibliothèque `scipy` pour trouver la racine de cette fonction dans l'intervalle spécifié par $[T_{min}, T_{max}]$.

## Résultats Obtenus

Nous avons utilisé des modèles pré-entraînés provenant du dépôt Github: <https://github.com/chenyaofo/pytorch-cifar-models>. Les valeurs trouvées pour température TS et température $T_{EC}$ sont respectivement 1.436 et 1.445, elle sont donc très proches. Nous remarquons que les deux méthodes donne des températures supérieures à 1 ce qui est cohérent avec le fait que les réseaux originaux étaient sûr-confiants.

| Modèle             | Score de Brier | ECE   |
|--------------------|----------------|-------|
| Modèle non calibré | 0.455          | 0.104 |
| Calibré avec TS    | 0.436          | 0.030 |
| Calibré avec TEC   | 0.436          | 0.028 |

: Résultats obtenus pour CIFAR-100 Resnet20 {#tbl-first}

On peux voir les résultats obtenus @tbl-first. D'après ces résultats on voit que les deux méthodes améliorent le score Brier et L'Expected Calibration Error et donnent des résultats très similaires.

Pour le modèle CIFAR-10 Resnet20 les valeurs trouvées pour température TS et température $T_{EC}$ sont respectivement 0.49 et 0.52 de plus l'erreur de classification vaut 0.077, elle est donc trés faible. Nous remarquons que cette fois les deux méthodes donne des températures infèrieures à 1.

| Modèle             | Score de Brier | ECE   |
|--------------------|----------------|-------|
| Modèle non calibré | 0.122          | 0.041 |
| Calibré avec TS    | 0.136          | 0.061 |
| Calibré avec TEC   | 0.135          | 0.060 |

: Résultats obtenus pour CIFAR-100 Resnet20 {#tbl-j}

On peux voir les résultats obtenus @tbl-j. D'après ces résultats on voit que les deux méthodes n'améliorent pas le score Brier et L'Expected Calibration Error et donnent des résultats moins bons que le modèle calibré, cela peut être du au fait que le modèle a une erreur de classification trés faible.

# Erreur de calibration attendue (ECE)

L'Expected Calibration Error (ECE) est une mesure d'évaluation de la calibration d'un modèle de réseau de neurones, particulièrement dans le contexte de la classification probabiliste (cf. @guo2017calibration). La calibration se réfère à la justesse des prédictions de probabilité du modèle, c'est-à-dire à quel point les probabilités prédites correspondent aux fréquences réelles des événements.

On dit qu'un algorithme de classification est calibré si la probabilité prédite $\hat{p}$, correspond à la probabilité réelle que la prédiction soit bonne. Ce qui revient mathématiquement à:
$$
P(\hat{y}=y|\hat{p}=p)=p, \quad \forall p\in [0;1]\enspace,
$$
où $\hat{y}$ est la classe prédite et y est la vraie classe. Dans tous les contextes pratiques, atteindre une calibration parfaite est impossible. L'erreur de calibration est une valeur qui représente la calibration du modèle sur l'ensemble des prédictions. Il s'agit de l'espérance mathématique de la différence entre la réalité et la confiance du modèle. On a donc:
$$
ECE=\mathbb{E}[P(\hat{y}=y|\hat{p}=p)-p]_{\hat{p}} \enspace.
$$
On a donc que une valeur faible de l'ECE indique une bonne calibration, tandis qu'une valeur élevée suggère une mauvaise calibration. En effet, une ECE faible indique que le modèle a une tendance à produire des probabilités proches des véritables probabilités d'appartenance à une classe.

La calibration d'un modèle peut être visualisée par un diagramme de fiabilité ( reliability diagram). Pour estimer la précision attendue à partir d'échantillons finis de taille $N$, il faut regrouper les prédictions en M intervalles (chacun de taille $\frac{1}{M}$). On considère l'intervalle $I_m=[\frac{m-1}{M},\frac{m}{M}]$ et $B_m$ l'ensemble des indices des échantillons dont la confiance de prédiction se situe dans l'intervalle $I_m$. Pour chaque groupe $B_m$ on calcule la précision (accuracy) qui correspond à la proportion d'échantillons correctement classés et la confiance moyenne:

$$
acc(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}{1}(y_i=\hat{y_i}) \enspace ,
$$

$$
conf(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}\hat{p_i} \enspace .
$$
Puisqu’il y a un nombre fini M de groupes, on calcule l’erreur de calibration comme suit :

$$
ECE=\frac{1}{N}\sum_{m=1}^{M}|B_m||acc(B_m)-conf(B_m)|\enspace .
$$
On peut donner un exemple assez simple, prenons 9 échantillons avec des probabilités estimées ou aussi appelés « confidences » ($\hat{p_i}$) pour prédire soit 0 ou 1. Si la probabilité $\hat{p_i}$ pour l'étiquette 0 est supérieure à 0,5 alors l'étiquette prédite sera 0. S'il est inférieur à 0,5, alors la probabilité sera plus élevée pour l'étiquette 1 et, par conséquent, l'étiquette prévue sera 1 (voir @tbl-second). La dernière colonne montre la véritable étiquette d'un échantillon i.



| échantillon (i) | Probabilité prédite pour le label 0| Probabilité prédite pour le label 1 | label prédit ($\hat{y_i}$) | Vrai label ($y_i$) |
|-----------------|------------------------------------|-------------------------------------|----------------------------|--------------------|
| 1               | 0.78                               | 0.22                                |  0                         |   0                |
| 2               | 0.36                               | 0.64                                |  1                         |   1                |
| 3               | 0.08                               | 0.92                                |  1                         |   0                |
| 4               | 0.58                               | 0.42                                |  0                         |   0                |
| 5               | 0.49                               | 0.51                                |  1                         |   0                |
| 6               | 0.85                               | 0.15                                |  0                         |   0                |
| 7               | 0.30                               | 0.70                                |  1                         |   1                |
| 8               | 0.63                               | 0.37                                |  0                         |   1                |
| 9               | 0.17                               | 0.83                                |  1                         |   0                |

: Situation {#tbl-second}

Pour déterminer le reste de la formule, nous devrons d'abord diviser nos échantillons en bacs. Seules les probabilités qui déterminent l'étiquette prévue sont utilisées dans le calcul de la ECE. Par conséquent, nous ne déposerons que des échantillons basés sur la probabilité maximale entre les étiquettes (voir @tbl-second). Pour garder l'exemple simple, nous avons divisé les données en 5 bacs, $M=5$. Avec B1 qui correspond aux probabilités prédites de 0 à 0.2, B2 qui correspond aux probabilités prédites de 0.2 à 0.4, B3 qui correspond aux probabilités prédites de 0.4 à 0.6, B4 qui correspond aux probabilités prédites de 0.6 à 0.8 et B5 qui correspond aux probabilités prédites de 0.8 à 1. Déposons maintenant les échantillons dans leurs bacs : dans B1 et B2 il n'y a aucun échantillons, dans B3 on a 4 et 5, dans B4 on a 8, 7, 2 et 1 et dans B5 on a 9, 6 et 3.

On va maintenant déterminer $conf(B_m)$, pour calculer $conf(B_m)$ nous prenons la somme des probabilités maximales estimées $\hat{p_i}$ pour chaque bac m dans @tbl-second, puis nous la divisons par la taille du bac |Bm|, on obtient : $con(B3)=\frac{0.58+0.51}{2}=0.545$, $con(B4)=\frac{0.78+0.64+0.7+0.63}{4}=0.6865$ et $con(B5)=\frac{0.92+0.85+0.83}{3}=0.8667$.

Il ne nous manque plus que $acc(B_m)$ qui correspond au nombre d'échantillons correctement prédit dans le bac m diviser par le nombre d'échantillons dans le bac. On trouve donc : $acc(B3)=0.5$, $acc(B4)=0.75$ et $acc(B5)=\frac{2}{3}$.

On peut don maintenant calculer l'ECE :

$$
ECE= \frac{2}{9}|0.5-0.545|   +   \frac{4}{9}|0.75-0.6865| + \frac{3}{9}|\frac{2}{3}-0.8667|=0.1044
$$
Dans notre petit exemple de 9 échantillons, nous nous avons une ECE de 0,1044. Un modèle parfaitement calibré aurait une ECE de 0.


On peut visualiser graphiquement cela à quoi ca correspond avec l'aide des diagrammes de fiabilité  (reliability diagrams en anglais) notre but ici est de montrer que le modèle calibrer avec la température $T_{EC}$ a une erreur de calibration plus faible que le modèle non calibré et une erreur de calibration relativement proche du modèle calibré avec TS. Voici les résultats pour un nombre de Bacs $M=10$:


::: {#fig-ECE layout-ncol=3 fig-pos='H'}
![Modèle non calibré](Figure_1.svg){width=160 #fig-1 }

![Modèle calibré avec TS](Figure_2.svg){width=160 #fig-2 }

![Modèle calibré avec TEC](Figure_3.svg){width=160 #fig-3 }

Diagrammes de fiabilité pour Cifar100-resnet20 
:::


En comparant @fig-1 avec @fig-2 et @fig-3 on remarque que c'est bien le cas. Pour interpréter ces Figures, on voit que sur @fig-1 le modèle a tendance a être sur-confiant, car les probabilités qu'il prédit (confidence) sont supérieurs à l'Expected accuracy qui est la précision réelle du modèle. Un modèle parfaitement calibré reste sur la diagonale, les barres rouges montre l'écart entre la confiance et la précision du modèle.

Pour la @fig-1 on voit que la confiance moyenne est largement supérieur à la précision moyenne du modèle ce qui montre clairement que le modèle est sur-confiant. Tandis qu'on voit que sur les @fig-2 et @fig-3  le modèle est mieux calibré et il donne des prédictions plus cohérentes au vu de la précision du modèle.

On a pour Cifar10-resnet20 :

::: {#fig-ECE layout-ncol=3 fig-pos='H'}
![Modèle non calibré](Figure_nc_c10.svg){width=160 #fig-4}

![Modèle calibré avec TS](Figure_ts_c10.svg){width=160 #fig-5}

![Modèle calibré avec TEC](Figure_ec_c10.svg){width=160 #fig-6}

Diagrammes de fiabilité pour Cifar10-resnet20 
:::







En comparant @fig-4 avec @fig-5 et @fig-6 on voit que pour ce modèle, les deux méthodes d'étalonnages n'améliore pas la calibration du modèle. Elles augmentent même l'ECE, cela peut être du au fait que l'on voit que le modèle est sous-confiant à un moment or ces méthodes semble etre efficace uniquement pour améliorer la calibration des modèles qui sont toujours sur-confiants.

# Score de Brier

Le score de Brier est une fonction de Score qui mesure l'exactitude des préditictions probabilistes. Pour les prédictions unidimensionnelles, elle est strictement équivalente à l'erreur quadratique moyenne aux probabilités prédites.

## Définition générale :

Dans le cas où une variable peut prendre plus de 2 valeurs. Le score de Brier est alors défini par :

$$
B_s = \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{m}(p_{i,j} - o_{i,j})^2
$$

- $n$ représente le nombre total d'instances de toutes les classes (nombre de sous vecteurs)
- $m$ représente le nombre total de classes possibles dans lesquels l'évènement peut tomber
- $p_{i,j}$ représente la probabilité prédite pour la classe $i$ pour i $\in \llbracket 1,n\rrbracket$
- $o_{i,j}$ vaut 1 si si la ième observation est de la catégorie $j$ et 0 sinon pour $j$
$\in \llbracket 1,m\rrbracket$

Le score de Brier peut être décomposé en 3 composantes additives : incertitude, fiabilité et résolution

$$
B_s = F - R + I
$$

- $I$ : terme d'incertitude qui prend en compte la dispersion des observations.

- $F$ : terme de fiabilité qui mesure dans quelle circonstances les probabilités prévues sont proches des probabilités réelles compte tenu d'une prévision. Si la fiabilité est de 0 la prévision est parfaitement fiable.
Par exemple si on regroupe tous les cas où 80% de probabilité de pluie était prévue, si il a plu 4 jours sur 5 on obtient ainsi une fiabilité parfaite.

- $R$ : terme de résolution qui mesure la distance entre les probabilités d'occurence


## Interprétation et décomposition :

Plus la valeur du score de Brier sera faible plus la prédiction sera bonne et une prévision parfaite obtiendra un score de 0. A l'inverse le plus mauvais score sera de 1.

Code de la fonction :


```{python}
# code de la fonction


def brier_score_f(predictions, observations):
    """
    Calcul du score de Brier.

    Arguments:
    predictions (liste de liste ou numpy array) Prédictions des probabilités
    pour chaque classe.
    observations (liste de liste ou numpy arrays): Observations réelles
    (0 ou 1 pour chaque classe).

    """
    n = len(predictions)
    m = len(predictions[0])
    bs = 0.0

    for i in range(n):
        for j in range(m):
            bs += (predictions[i][j] - observations[i][j]) ** 2

    bs /= (n * m)
    return bs

```

Exécution :

```{python}
# Exemple :

predictions = [[0.1, 0.9, 0.8, 0.3],[0.4,0.9,0.2,0.7]]
observations = [[0, 1, 1, 0],[1,1,0,1]]

bs = brier_score_f(predictions, observations)
print("Score de Brier :", bs)


```




# Bibliographie

::: {#refs}
:::



