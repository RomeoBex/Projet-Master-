\documentclass{article}
\title{La Regression logistique}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{dsfont}




\begin{document}
\maketitle


La régression logistique est une technique de modélisation statistique utilisée pour prédire la probabilité qu'une variable descriptive binaire prenne l'une des deux valeurs possibles (0 ou 1), on peut noter Y cette variable ( elle appartient à $\{0, 1\}^n$) en fonction d'un ensemble de variables explicatives que l'on note $X = \begin{bmatrix} \bm{x}_1 \\ \vdots \\ \bm{x}_n \end{bmatrix} \in \mathbb{R}^{n \times p},
$ avec n observations et p variables. C'est une méthode couramment utilisée en apprentissage automatique et en statistiques.

La régression logistique a une interprétation probabiliste, elle permet de modéliser $P(Y = 1 | \bm{x})$,où $\bm{x}\in \mathbb{R}^{p}$.

En utilisant la loi de Bayes et le fait que : 
\begin{align*}
    P(\bm{x}) &= P(\bm{x}|Y=1)P(Y=1) + P(\bm{x}|Y=0)P(Y=0) \\
\end{align*}
nous avons :

\begin{align*}
P(Y=1|\bm{x}) &= \frac{P(\bm{x}|Y=1)P(Y=1)}{P(\bm{x}|Y=1)P(Y=1) + P(\bm{x}|Y=0)P(Y=0)} \\
&= \frac{1}{1 + \frac{P(\bm{x}|Y=0)P(Y=0)}{P(\bm{x}|Y=1)P(Y=1)}} \\
&= \frac{1}{1 + \frac{P(Y=0|\bm{x})}{P(Y=1|\bm{x})}} \\
\end{align*}

On note $f(\bm{x}) := \log\left(\frac{P(Y=1|\bm{x})}{P(Y=0|\bm{x})}\right)$.


On a ainsi \( P(Y=1|\bm{x}) =: \sigma(f(\bm{x})) \) avec \( \sigma(z) = \frac{1}{1 + e^{-z}} \).

La fonction \( \sigma \), appelée fonction logistique, satisfait les propriétés suivantes :
\begin{align*}
    \sigma(-z) &= 1 - \sigma(z) \\
    \frac{d\sigma(z)}{dz} &= \sigma(z) \sigma(-z).
\end{align*}

L'intérêt de la fonction logistique réside dans sa capacité à transformer une fonction \( f \) à valeurs dans \( \mathbb{R} \) en une probabilité comprise entre 0 et 1.


La régression logistique revient en fait à supposer que \( f \) est linéaire de la forme \( f : \bm{x} \mapsto \bm{\theta}^\top \bm{x} \) avec \( \bm{\theta} \in \mathbb{R}^p \).


Sous cette hypothèse, la règle de classification est simplement :
\[
\begin{cases}
\text{si } \bm{\theta}^\top \bm{x}\leq 0 , \text{ on étiquette 0 au point } \bm{x} \\
\text{si } \bm{\theta}^\top \bm{x} > 0, \text{ on étiquette 1 au point } \bm{x}
\end{cases}
\]

On obtient donc :
\begin{align*}
    P(Y=1|\bm{x}) &= \sigma(\bm{\theta}^\top \bm{x}) \\
    P(Y=0|\bm{x}) &= 1- \sigma(\bm{\theta}^\top \bm{x}) = \sigma(-\bm{\theta}^\top \bm{x})
\end{align*}

Le but maintenant est d'estimer $\bm{\theta}$. Nous avons $(x_i, y_i)_{1 \leq i \leq n}$  où \(x_i \in \mathbb{R}^p\) et \(y_i \in \{0,1\}\) qui constitue un échantillon de taille \(n\).
. On a alors :

\[ P(Y = y_i | \bm{x} = x_i) = \sigma(\bm{\theta}^\top x_i)^{y_i} \sigma(-\bm{\theta}^\top x_i)^{1-y_i}. \]

La log-vraisemblance s'exprime alors de cette manière :

\begin{align*}
    L(\bm{\theta}) &= \sum_{i=1}^{n} \log\left( \sigma(\bm{\theta}^\top x_i)^{y_i} \sigma(-\bm{\theta}^\top x_i)^{1-y_i} \right) \\
                   &= \sum_{i=1}^{n} l(\bm{\theta}^\top x_i, y_i).
\end{align*}

où l est définie comme étant la fonction de perte logistique. Il faut ensuite  avoir recours à des algorithmes itératifs (descente de gradient, méthode de Newton,...) pour trouver  $\bm{\hat{\theta}}$.

On peut passer du cadre binaire au cadre multi-classes avec \(K\) classes, par exemple, c'est-à-dire en ayant \(Y\) appartenant à \(\llbracket 1, K \rrbracket^{n}\). De nouveau, on modélise les probabilités conditionnelles des classes, ou plutôt leur log-ratio, par des quantités linéaires :
$\log\left(\frac{P(Y=k|\bm{x})}{P(Y=K|\bm{x})}\right) = \theta_k^\top \bm{x}_i
$ pour \(k \in \llbracket 1, K-1 \rrbracket\) et $\theta_{k} \in \mathbb{R}^p$


On a alors pour paramètre globale: \(\theta \in [\theta_1, \ldots, \theta_K] \in \mathbb{R}^{p\times K}\) et pour \(k \in \llbracket 1, K \rrbracket\):

\begin{align*}
    P(Y=k | \bm{x}) &= \frac{\exp(\langle \boldsymbol{\theta}_k, \bm{x} \rangle)}{\sum_{l=1}^{K} \exp(\langle \boldsymbol{\theta}_l, \bm{x} \rangle)}, \\
    &= \frac{\exp(\boldsymbol{\theta}_k^T \bm{x})}{\sum_{l=1}^{K} \exp(\boldsymbol{\theta}_l^T \bm{x})}
\end{align*}


On peut écrire cette égalité sous forme vectorielle en utilisant la notation softmax, on a alors :
$(P(Y=k | \bm{x}))_{k=1,...,K}=softmax(\theta_1^T\bm{x},..,\theta_K^T\bm{x})$

Pour la régression softmax, la log-vraisemblance peut être exprimée comme suit :

\begin{align*}
L(\bm{\theta}) &= \sum_{i=1}^{n} \sum_{k=1}^{K} \mathds{1}(y_i = k) \log\left(\frac{e^{\theta_k^\top x_i}}{\sum_{l=1}^{K} e^{\theta_l^\top x_i}}\right) \\
\end{align*}

On utilise ensuite des méthodes algortihmiques pour trouver    $\bm{\hat{\theta}}$.



\end{document}
