\documentclass{article}
\title{La Regression logistique}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm} 




\begin{document}
\maketitle


La régression logistique est une technique de modélisation statistique utilisée pour prédire la probabilité qu'une variable descriptive binaire prenne l'une des deux valeurs possibles (0 ou 1), on peut noter Y cette variable ( elle appartient à $\{0, 1\}^n$) en fonction d'un ensemble de variables explicatives que l'on note $X = \begin{bmatrix} \bm{x}_1 \\ \vdots \\ \bm{x}_n \end{bmatrix} \in \mathbb{R}^{n \times p},
$ avec n observations et p variables. C'est une méthode couramment utilisée en apprentissage automatique et en statistiques.

La régression logistique a une interprétation probabiliste, elle permet de modéliser $P(Y = 1 | \bm{x})$,où $\bm{x}\in \mathbb{R}^{p}$.

En utilisant la loi de Bayes et le fait que : 
\begin{align*}
    P(\bm{x}) &= P(\bm{x}|Y=1)P(Y=1) + P(\bm{x}|Y=0)P(Y=0) \\
\end{align*}
nous avons :

\begin{align*}
P(Y=1|\bm{x}) &= \frac{P(\bm{x}|Y=1)P(Y=1)}{P(\bm{x}|Y=1)P(Y=1) + P(\bm{x}|Y=0)P(Y=0)} \\
&= \frac{1}{1 + \frac{P(\bm{x}|Y=0)P(Y=0)}{P(\bm{x}|Y=1)P(Y=1)}} \\
&= \frac{1}{1 + \frac{P(Y=0|\bm{x})}{P(Y=1|\bm{x})}} \\
\end{align*}

On note $f(\bm{x}) := \log\left(\frac{P(Y=1|\bm{x})}{P(Y=0|\bm{x})}\right)$.


On a ainsi \( P(Y=1|\bm{x}) =: \sigma(f(\bm{x})) \) avec \( \sigma(z) = \frac{1}{1 + e^{-z}} \).

La fonction \( \sigma \), appelée fonction logistique, satisfait les propriétés suivantes :
\begin{align*}
    \sigma(-z) &= 1 - \sigma(z) \\
    \frac{d\sigma(z)}{dz} &= \sigma(z) \sigma(-z).
\end{align*}

L'intérêt de la fonction logistique réside dans sa capacité à transformer une fonction \( f \) à valeurs dans \( \mathbb{R} \) en une probabilité comprise entre 0 et 1.


La régression logistique revient en fait à supposer que \( f \) est linéaire de la forme \( f : \bm{x} \mapsto \bm{\theta}^\top \bm{x} \) avec \( \bm{\theta} \in \mathbb{R}^p \).


Sous cette hypothèse, la règle de classification est simplement :
\[
\begin{cases}
\text{si } \bm{\theta}^\top \bm{x}\leq 0 , \text{ on étiquette 0 au point } \bm{x} \\
\text{si } \bm{\theta}^\top \bm{x} > 0, \text{ on étiquette 1 au point } \bm{x}
\end{cases}
\]

On obtient donc :
\begin{align*}
    P(Y=1|\bm{x}) &= \sigma(\bm{\theta}^\top \bm{x}) \\
    P(Y=0|\bm{x}) &= 1- \sigma(\bm{\theta}^\top \bm{x}) = \sigma(-\bm{\theta}^\top \bm{x})
\end{align*}

Le but maintenant est d'estimer $\bm{\theta}$. Nous avons $(x_i, y_i)_{1 \leq i \leq n}$  où \(x_i \in \mathbb{R}^p\) et \(y_i \in \{0,1\}\) constitue un échantillon de taille \(n\).
. On a alors :

\[ P(y = y_i | \bm{x} = x_i) = \sigma(\bm{\theta}^\top x_i)^{y_i} \sigma(-\bm{\theta}^\top x_i)^{1-y_i}. \]

La log-vraisemblance s'exprime alors de cette manière :

\begin{align*}
    L(\bm{\theta}) &= \sum_{i=1}^{n} \log\left( \sigma(\bm{\theta}^\top x_i)^{y_i} \sigma(-\bm{\theta}^\top x_i)^{1-y_i} \right) \\
                   &= \sum_{i=1}^{n} l(\bm{\theta}^\top x_i, y_i).
\end{align*}

où l est définie comme étant la fonction de perte logistique. Il faut ensuite  avoir recours à des algorithmes itératifs (descente de gradient, méthode de Newton,...) pour trouver  $\bm{\hat{\theta}}$.


\end{document}
